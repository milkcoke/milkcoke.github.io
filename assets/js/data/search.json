[ { "title": "Batch command in redis", "url": "/posts/Batch_Command/", "categories": "Redis", "tags": "Redis", "date": "2023-12-02 00:00:00 +0900", "snippet": "why to use batch?When we have to send a lot of command to redis Redis server have to process commands and replies about them. In request - response mechanism, thereâ€™s also â€˜Round Trip Timeâ€™.Round Trip Time (RTT)Why RTT makes process slow?internet hopsIf client and server communication using internet protocol Thereâ€™s many hops between them. According to region, routing, many differences exist.context switchingRedis client - server has TCP connection. When sending command (client) and receiving the command (server), thereâ€™s socket read() , write() blocking system call. So, a lot of context switching between user and kernel area in the server.How to use pipeline batchRedis has â€˜pipelineâ€™ command for batch commands.You should consider request reasonable number commands on it.While the client sends commands using pipelining, the server will be forced to queue the replies, using memory.If massive command in a single pipeline, thereâ€™s possibility of peeking memory usage on redis server.References Redis pipelining - redis offical docs" }, { "title": "Lock in redis", "url": "/posts/Lock/", "categories": "Redis", "tags": "Redis", "date": "2023-09-15 00:00:00 +0900", "snippet": "Lockingwe could use lock for overcoming optimistic lock solutionExample of locking solutionBid system example refer previous post" }, { "title": "Concurrency in redis", "url": "/posts/Concurrency/", "categories": "Redis", "tags": "Redis", "date": "2023-09-12 00:00:00 +0900", "snippet": "In many cases, concurrency could be ignored.It depends on your business requirement.ProblemRace condition depends on sequence of commands. Result differ according to it.Why this happened? Thereâ€™s gap between read and update.Solution(1) Use atomic commandex)HSETNX, HINCRBY It removes gap between read and command (2) Use transactionRedis transaction using MULTI and WATCH keyword for guaranteeing commands are executed sequentially. However, it doesnâ€™t support rollback.Transaction exampleMULTI command enqueue all commands until EXEC command to be executed. So, commands are to be executed sequentially one by one.&gt;&gt; MULTI-- \"OK\"&gt;&gt; SET color red&gt;&gt; SET color blue-- \"QUEUED\"&gt;&gt; EXEC-- [\"OK\", \"OK\"]WATCH command provides optimistic lock. It allow only single command operation. And make other transactions fail.-- optimistic lock&gt;&gt; WATCH color&gt;&gt; SET color red&gt;&gt; MULTI&gt;&gt; SET color blue&gt;&gt; SET color green&gt;&gt; EXEC-- nil (failed to execute transaction)&gt;&gt; SET color yellow-- success color set as `yellow` since `EXEC` call `UNWATCHED` implicitlyConcurrency issue is not always problem To Do app status update Calender app Only one user can access (not shared resource)Like those app, concurrency issue is not critical.Cons of WATCH solutionSuppose bid system.bid condition is â€œbid amount must be greater than item priceâ€. Thereâ€™s error case when applying WATCH solution. Let me show you example,when original Item price is 5$, If first request amount 10$ and second request amount 15$ Both request occur almost simultaneously, first request succeed but second request is going to fail. This is error case since second request amount 15$ is greater than 10$ of first request.For Guaranteeing second request success, we should implement retry logic. From this , A lot of load occurs on redis server since thereâ€™s many fail cases for short time.Summary cons of WATCH solutionWe call it WATCH solution as optimistic lock. Error case Not scalable Especially mass requests occurs, results in a lot of fail transactions and load on redis server. Redis cluster doesnâ€™t support WATCH For resolving this problem, we can use distributed lock.You can see lock in redis this post" }, { "title": "List of redis", "url": "/posts/List/", "categories": "Redis", "tags": "Redis", "date": "2023-09-08 00:00:00 +0900", "snippet": "When to use Append only or prepend only data Time series dataex) temperature readings, stock values Non-stop overtime! Need to only fetch last or first element is reuiqred only. But this requirement is also implemented in sorted set. So use sorted set instead of list Data is organized by insertion order or time orderDonâ€™t use You need to apply some filtering criteria Data requires sort by some attribute. In List, thereâ€™s no filtering commandTipsThis is not an array!!! This is implemented as doubly linked list. Store an ordered list of strings." }, { "title": "HyperLogLog", "url": "/posts/Hyper_loglog/", "categories": "Redis", "tags": "Redis", "date": "2023-09-07 00:00:00 +0900", "snippet": "HyperLogLogHyperLogLog is an algorithm for the count distinct problem. Redis provides HyperLogLog algorithm string data It only uses 12 KB.When to use?Guarantee approximate uniqueness when handle large dataset using small memoryHow to use?PFADDExample-- It's like a set, doesn't actually store true valuePFADD &lt;string | number&gt;Duplicate -&gt; 0 (false) Unique -&gt; 1 (true)PFCOUNT-- Get the approximate count of key.PFCOUNT &lt;key&gt;Practical exampleRequirementsSuppose we need to guarantee uniqueness of item views. So, Itâ€™s not allowed to increment many view counts by same user.Simple solutionWe can think about simple solution using SET data structure keeping username of items:viewsThis solution guarantee exact number of view count. However, requires many memory usage even though itâ€™s simple featureHyperLogLog SolutionThis solution requires only 12KB memory usage.However, it has error rate approximately 0.81%.This solution is appropriate than SET solution since itâ€™s important reduce redis memory usage and itâ€™s not much important counting views exactly.ConclusionConsider using HyperLogLog for ensuring uniqueness with some error. It depends on your feature. Itâ€™s appropriate when handle count views, likes etc.Donâ€™t use it when you need guaranteeing exact uniqueness like username , userId , email etc." }, { "title": "Loading relational data", "url": "/posts/Loading_relational_data/", "categories": "Redis", "tags": "Redis", "date": "2023-09-06 00:00:00 +0900", "snippet": "Thereâ€™s two solution for getting loading relational data(1) RANGE based query + GET queryLet me show you example code.export async function itemsByEndingTime( order: 'DESC' | 'ASC' = 'DESC', offset: number = 0, count: number = 10): Promise&lt;Item[]&gt; { // First range-based query const itemIdList = await redisClient.zRange(generateItemEndingAtKey(), Date.now(), '+inf', { BY: 'SCORE', LIMIT: { offset: offset, count: count } }); // Pipeline with two separated commands const results = await Promise.allSettled(itemIdList.map(itemId =&gt; redisClient.hGetAll(generateItemKey(itemId)))); if (isExistsError(results)) { throw new Error(\"Error occurs when sorting items by ending time\"); } return results .filter(result =&gt; result.status === 'fulfilled') .map((item: PromiseFulfilledResult&lt;any&gt;, index)=&gt; { return deserializeItem(itemIdList[index], item.value); });}ProsSimple solutionConsThis solution requires two redis operation (two network round trip)(2) SortUse the SORT command for loading relational data.When to use Sort data types on sets, sorted sets, lists Apply different sort criteria.How to operate SORT?-- [Requirement] Sort book name order by book's year-- (1) 'SORT' Extract all the members from books:likes (sorted set)-- (2) 'BY' Take each different members and going to insert them into this template-- good -&gt; 1950, bad -&gt; 1930, ok -&gt; 1940-- (3) 'GET' # (pound) means that insert original ID whatever the origiannl member was-- (4) 'GET' repeat that same thing in (2) . Loop through each member and get the property (key)-- (5) 'GET' loop all year!-- Return members with original ID and joined titleSORT books:likes BY books:*-&gt;year GET # GET books:*-&gt;title GET books:*-&gt;yearOriginal dataSort internally with join GETResultNo sort Use nosnort keyword instead of arbitrary key name.If not exist key on â€˜BYâ€™ clause, SORT operation doesnâ€™t sort. Just follow original data sequence. In this situation, you could use nosort keyword-- [No Sort] Use 'nosort' keyword-- Apply no sort (In this example, followed by sorted set order status)SORT books:likes BY nosort GET # GET books:*-&gt;titleProsJust one network round trip. Single operationConsRelatively complex query and code." }, { "title": "Redis sorted set", "url": "/posts/Sorted_Set/", "categories": "Redis", "tags": "Redis", "date": "2023-09-02 00:00:00 +0900", "snippet": "When to use Enforce uniqueness of elements Enforce sort Needs range based query Tabulating a collection in order Creating relationships between entities sorted by some criteria Range based queryZ* operations range has inclusive semantic. And those has zero-based index.ZCOUNTinclusive semantic (default)ZADD products 5 mouseZADD products 10 keyboardZADD products 25 monitorZADD products 40 operating_systemZADD products 100 cpuZCOUNT products 10 25-- 2 (keyboard: 10, monitor: 25)exclusive semanticZCOUNT products (10 25-- 1 (monitor: 25)ZRANGEZRANGE is very useful and often used operation.ZRANGE products 1 1 WITHSCORES-- [keyboard, 10]score-based searchZRANGE products 10 30 BYSCORE WITHSCORES-- [keyboard, 10, monitor, 25]index-based search-- Reverse order before queryZRANGE products 1 1 REV WITHSCORES-- [operating_system, 40]TipsRedis is key value based NoSQL. Itâ€™s allowed to use duplication for reducing query and enhancing performance.Suppose we need provide most views on items.We donâ€™t need only use Hash data which has ${item:id} as key. We can also use Sorted Set for reducing query and get the highest views items just one query items:views like as shown below." }, { "title": "Set data type", "url": "/posts/Set_datatype/", "categories": "Redis", "tags": "Redis", "date": "2023-09-01 00:00:00 +0900", "snippet": "When to use Enforce uniqueness of elements Creating relationship between other entity Common elements between other entity Guarantee operation idempotence Order of elements doesnâ€™t matter. ex) ip black list.TipsWhy do we need store operation? SINTERSTORE SDIFFSTORE SUNIONSTOREReduce the number of execution those operation. Those operationsâ€™ cost is expensive." }, { "title": "Which data is to be cached", "url": "/posts/Which_data_cache/", "categories": "Redis", "tags": "Redis", "date": "2023-08-31 00:00:00 +0900", "snippet": "Consideration design on redis What data should be fetched Data structure Often accessed and common data among users This is common strategy when choosing cache data. Not often changed data If you chosen cache data which is often changed data (especially for personalized data), in-memory space should be reduced very quickly. \\ExampleSuppose you have to select which web page should be cached using redis. You are administrator and owner of web site.Static web page is better choice. These are the reason. Often accessed data Smaller size data Data not changed easily Not related business logic" }, { "title": "Serialize and Deserialize", "url": "/posts/Serialize_hash/", "categories": "Redis", "tags": "Redis", "date": "2023-08-31 00:00:00 +0900", "snippet": "RequirementsWe have to provide serialize and deserialize in redis hash.Because, serialize doesnâ€™t require id in values. And itâ€™s not appropriate storing specific format and data in redis because of time zone, data storage cost. However, deserialized data requires id and specific format.How to resolve it?Define serialize and deserialize helper function.For about Javascript Date , id (PK).Serialize requirements Remove id for saving storage cost Client already knows what the key is for searching. Transform from specific date object to string type which is searched easily in string.Example codefunction serializeItem(attrs: CreateItemAttrs) { return { // epoch milliseconds // insert as 'string' instead of Date object type. createdAt: attrs.createdAt.toMillis(), endingAt: attrs.endingAt.toMillis() };}Deserialize requirements Transform data type from empty string('') to undefined or null from string â€˜5â€™ to number 5 from string â€˜March-5-2023â€™ to Date object Insert id in objectExample// Include id and convert string to date object, number etc..function deserializeItem(id: string, item: { [key: string]: string }): Item { return { // item is not object type // so it's not possible to use spread operator. id: id, name: item.name, ownerId: item.ownerId, imageUrl: item.imageurl, description: item.description, createdAt: DateTime.fromMillis(Number(item.createdAt)), endingAt: DateTime.fromMillis(Number(item.endingAt)), views: Number(item.views), likes: Number(item.likes), price: Number(item.price), bids: Number(item.bids), highestBidUserId: item.highestBidUserId };}" }, { "title": "Tips about Hash commands", "url": "/posts/Hash_data/", "categories": "Redis", "tags": "Redis", "date": "2023-08-31 00:00:00 +0900", "snippet": "When to use hash?Store key - value entity. Itâ€™s similar row in traditional databaseReasons to use hashes The row has many attributes A collection of records have to be sorted many different ways. Often need to be accessed single row at a timeEX) user, item, session entities.Donâ€™t use hashes when Thr row has few attributes Row is only for counting or enforcing uniqueness. Used only for creating relations between different entity Time series dataEX) views, like, bidsDonâ€™t use null and undefined, nested objectUnlike, traditional database. Redis doesnâ€™t allow insert null or undefined as value.TL;DR; Donâ€™t use nested object and null, undefined as it is.Iâ€™ll show the example. This code may lead to error.Exampleconst run = async (): Promise&lt;void&gt; =&gt; { await client.hSet('car', { color: 'red', year: 1950, engine: { cylindar: 8 }, owner: null, service: undefined }); const car = await client.hGetAll('car'); console.dir(car);};run();ErrorTypeError: Cannot read properties of null (reading 'toString')TypeError: Cannot read properties of undefined (reading 'toString')Why error occurs?In HSET, redis client internally .toString() method is called before inserting value regardless of input data type.when trying null.toString() not allowed.Also, nested object value is converted to [Object object]{ color: 'red', year: '1950', engine: '[object Object]'}How to resolve it?Use empty string instead of null or undefined.const run = async (): Promise&lt;void&gt; =&gt; { await client.hSet('car', { color: 'red', year: 1950, engine: { cylindar: 8 }, owner: null || '', service: undefined || '' }); const car = await client.hGetAll('car'); console.dir(car);};run();HGETALL has gatchaBe careful using HGETALL on your application.Maybe you are to expect null or undefined if key doesnâ€™t exists.However, redis return {} (I call it empty object)Let me show you example.example codeconst run = async (): Promise&lt;void&gt; =&gt; { const car = await client.hGetAll('car#noteixst'); if (!car) { // This code is not executed console.log(\"car doesn't exist\"); return; } console.dir(car);};run();result{}This default behavior is different to traditional database.How to resolve it? You should use custom empty object check method.const run = async (): Promise&lt;void&gt; =&gt; { const car = await client.hGetAll('car#noteixst'); if (isEmptyObject(car)) { console.log(\"car doesn't exist\"); return; } console.dir(car);};run();function isEmptyObject(obj: object) { return Object.keys(obj).length === 0;}resultcar doesn't exist" }, { "title": "Redis multi key and range operation", "url": "/posts/Why_use_range_operation/", "categories": "Redis", "tags": "Redis", "date": "2023-08-30 00:00:00 +0900", "snippet": "Multi key operationMSETSet multi key - value pairsredis-cli&gt; MSET key1 value1 key2 value2MGETGet multi values from multi keys order by keyredis-cli&gt; MGET key1 key2redis-cli&gt; [value1, value2]GETRANGE, SETRANGE Two indexes means between range [start, end].redis-cli&gt; SET name falcon# from 0 index to 2 index (including 2)redis-cli&gt; GETRANGE 0 2 # \"fal\"Why to use them?It doesnâ€™t look useful. However, suppose we use traditional database table including multiple columns. Thereâ€™s some operations when we use update partial properties some rows. Fetch some column of a single row Update some column of a single row Create some rows with multiple columnsEspecially, when using hashtable with encode value (ex. alphabet)ExampleINCR / DECRThese operations are supported for number type. It seems that not necessary because we can implement using GET + SETWhy to use them? Reduce network round trip It costs two round trip if GET + SET operations are used. INCR / DECR require single operation Prevent from race condition Race condition would occur. when trying SET operation simultaneously by multiple servers. UsingINCR / DECR operation resolve this problem since the Redis is based on single thread." }, { "title": "Redis Overview", "url": "/posts/Overview/", "categories": "Redis", "tags": "Redis", "date": "2023-08-29 00:00:00 +0900", "snippet": "Why use redis Faster than other DB REDIS is abbreviation of REmote DICtionary Server Simple and organized data structure Redis support strings, hashes, set, sorted set, list Simple feature set comparing to other DB In redis, thereâ€™s no triggers, data schema, FK constraints, Transaction rollback etc..How to learn from redis.io/commandsIn same bracket [] we have to use only single option not multiple options.Why expiration is required?Redis is memory based db and it origins from cache. Delete old cache Guarantee data consistency between redis and traditional database (original data) Prevent from running out of memory Expiration is for caching!" }, { "title": "Locking", "url": "/posts/Locking/", "categories": "database", "tags": "database", "date": "2023-07-25 00:00:00 +0900", "snippet": "Exclusive LockUsually, write.Itâ€™s like a mutex lockLifetimeIn a single transactionOnly one transaction can get exclusive lock.Shared LockUsually, read.LifetimeIn multiple transactionsMultiple transactions can get shared locks.Dead LockBEGIN TRANSACTION;INSERT INTO testVALUES 20INSERT INTO testVALUES 30BEGIN TRANSACTION;INSERT INTO testVALUES 30INSERT INTO testVALUES 20-- DeadLock OccursDB Rejects transaction and fail when deadlock occurs.Two Phase LockUsually used with snapshot isolation level.It resolves Lost update and Phantom Read.Why to useAvoid race conditionBooking problem Two phase locking is robust enough to block all possible race conditions among concurrent transactions including phantom reads, lost updates and write skew.Readers donâ€™t block writers and Writers donâ€™t block readers.ExampleA book seat 1 which is empty in SELECT transactionB book seat 1 empty in SELECT transactionA succeed book seat 1 in transaction and COMMIT!B succeed book seat 1 in transaction and COMMIT!=&gt; Lost Update â€œAâ€References https://medium.com/double-pointer/transactions-for-system-design-interview-8-two-phase-locking-fcb74458785a https://medium.com/@hnasr/postgres-locks-a-deep-dive-9fc158a5641c" }, { "title": "MongoDB Internal Architecture", "url": "/posts/MongoDB/", "categories": "nosql", "tags": "nosql", "date": "2023-07-24 00:00:00 +0900", "snippet": "History of MongoDB EngineMMAPV1 (~v2.x)Map based data structure.So it has disadvantage about range-based query, concurrent update.ID is consists fo Diskloc(collection data file name) + offset (bytes)WiredTiger (~v5.1)Support Document level locking so itâ€™s able to update 2 document concurrently.Primary index _id and secondary indexes have been changed to point to recordId (a 64 bit integer) instead of the Diskloc. Itâ€™s like a Postgres which point to rowID.Support compressed BSON. =&gt; Lower I/O =&gt; Lower Page =&gt; Getting more document -&gt; Efficient Operation!\\[Time Complexity = Log(N) * Log(N)\\]Current MongoDB (v5.2 ~)Uses clustered index like MySQL.But itâ€™s not enforced.Secondary indexes are easy to bloat since _id is large (12 bytes). It no longer point to recordId.RecordId should point to _id which is 12 bytes.Limit of Document Size Maximum: 16MB Minimum: 255KBIndex on MongoDBSupport Secondary and Primary index.Itâ€™s easy to bloat size of secondary index since ObjectId is 12 bytes.ObjectID (_id)12 bytes PK." }, { "title": "Don't use DB as Container", "url": "/posts/Container/", "categories": "Database", "tags": "database", "date": "2023-05-07 00:00:00 +0900", "snippet": "DB as container is not good approach(1) StatelessContainers are designed for stateless processes. DB is a stateful system.You have to manage volume. This is so tricky since concurrency and performance issues.(2) PerformanceContainers are not designed for high-performance workloads such as databases. Containers can be slower than running databases on bare metal or virtual machines because of the additional overhead of running the container environment.(3) PersistenceDatabase data must be backed up regularly. Native DB Engine supports this feature, if you use container and volume, you should ensure support that.Separate strategy between development and production(1) Database nameSeparate only database name between development and production.In intial phase, you could use this approach saving cost.ProsEasy to setSaving costConsState is coupled. If cluster is down. Both environment down simultaneously(2) Database engineSeparate database engine completely.ProsState is also decoupled.ConsItâ€™s more difficult to manage than first approachexpensive than former" }, { "title": "Query execution plan", "url": "/posts/Query_Execution_Plan/", "categories": "Database", "tags": "database", "date": "2023-01-19 00:00:00 +0900", "snippet": "PostgreSQL Type Description Seq scan Full scan When index doesnâ€™t exist, or expected rows are large (greater than 5~10%) Index scan Search by key and retrieves data located in heap (Always RID Lookup) Index only scan Search only keys (covering index) Postgres uses only non-clustered index (secondary index). RID lookup always occurs when executing index scan.SQL Server Type Description Table scan Full scan when table without a clustered index is accessed. Clustered Index scan Clustered index exists but query canâ€™t use non-clustered index Clustered Index seek Scan limited set of rows using B tree structure clustered index. Non-Clustered Index scan Table has non-clustered index but query requires accessing a large amount of data across particular index Non-Clustered Index seek Query accesses data using B+ tree index (No Lookup, Covering index) Lookup Query requires accessing not exists in B+ tree =&gt; RID/Key lookup =&gt; Random I/O occurs. ğŸ“ Clustered Index scan is not good because even though the table has clustered index, itâ€™s not used for searching really. This would lead to a performance degradation. In SQL Server, Index scan and table scan are not so different in terms of performance.ğŸ”— References Youtube SQL Server clustered index operators SQL Server non-clustered index operators Key lookup and RID lookup in SQL Server" }, { "title": "Database Partitioning", "url": "/posts/Partitioning/", "categories": "cache", "tags": "cache", "date": "2022-11-28 00:00:00 +0900", "snippet": "Whatâ€™s PartitioningSplit the big table into multiple logical tables in the same table.When to use Partitioning?If table size is bigger and bigger , Indexes should being large, the queries could slower than beforePartition the table with index! =&gt; Query to small size indexes so targeting specific partitioned table! =&gt; Faster than whole index scanning!Horizontal vs Vertical PartitioningHorizontalSplit rows range, list, hash based.VerticalSplits columns partitions (Not recommended)Partitioning TypesRangeDates, ids(e.g. year of logDateTime , userId)ex) If you donâ€™t use log date before 2015 years, You can archive old partitions to cheap storages.ListDiscrete values ex) zip codesHashHash function decides where to store the data.Cassandra, DynamoDB uses this strategy as default.âš ï¸Partitioning is not Sharding Sharding splits big table into multiple physical tables across multiple DB Servers.On the other hand, Partitioning have multiple logical tables in same table in same server.Example partitioning in postgresNormal table1. Create test tableCREATE TABLE grades( id integer generated always as identity constraint grades_pk primary key, grade integer not null);2. Insert 10 million rows.INSERT INTO grades(grade)-- key range (0 ~ 99)SELECT floor(random() * 100)-- generate 10 million rowsFROM generate_series (0, 10000000);3. Create index on gradeCREATE INDEX grades_grade_index ON grades(grade);SELECT COUNT(*)FROM gradesWHERE grade = 30;-- 1001034. query on tableNon partitioning result (equal query)-- Let's analyze query execution planEXPLAIN ANALYZE SELECT COUNT(*)FROM gradesWHERE grade = 30;-- on base table's index!Index Only Scan using grades_grade_index on grades Heap Fetches: 0 Execution Time: 20.612 msNon partitioning result (range-based query)EXPLAIN ANALYZE SELECT COUNT(*)FROM gradesWHERE grade BETWEEN 30 AND 35;Parallel Index Only Scan using ... Heap Fetches: 0Execution Time: 42.094 ms ğŸ’¡ You must execute above first, and follow next section since itâ€™s reused in this section above the grades table.Partitioning Table1. create table with partitioncreate table grades_partition( id integer generated by default as identity, grade integer not null) partition by RANGE (grade)CREATE TABLE g0025 (LIKE grades_partition INCLUDING INDEXES);ALTER TABLE grades_partition ATTACH PARTITION g0025 FOR VALUES FROM (0) to (25);CREATE TABLE g2550 (LIKE grades_partition INCLUDING INDEXES);ALTER TABLE grades_partition ATTACH PARTITION g2550 FOR VALUES FROM (25) to (50);CREATE TABLE g5075 (LIKE grades_partition INCLUDING INDEXES);ALTER TABLE grades_partition ATTACH PARTITION g5075 FOR VALUES FROM (50) to (75);CREATE TABLE g75100 (LIKE grades_partition INCLUDING INDEXES);ALTER TABLE grades_partition ATTACH PARTITION g75100 FOR VALUES FROM (75) to (100);2. Insert 10 million rows from grades tableINSERT INTO grades_partitionSELECT *FROM grades; Everytime you insert row to grade_partition table, The DB will decide which partitoin that row goes to based on the value of grade.SELECT COUNT(*)FROM g0025;-- count: 2499474-- only partitioned rows are showned!3. Create IndexOnly create index on leader partition table It will crate same index on all the partitions!CREATE INDEX grades_partition_idx ON grades_partition(grade);4. Query on tableEXPLAIN ANALYZE SELECT COUNT(*)FROM grades_partitionWHERE grade = 30; -- 30 on partition 'g2550'Partitioned result-- search index in partition 'g2550'Index Only Scan using g2550_grade_idx on g2550 grades_partitionExecution Time: 17.511 ms Even if not shown difference between non-partition since my M1 Macbook Pro have enough RAM memory and performance. There should be difference between them if thereâ€™s limit of main memory or lower performance environment. Large index size (Non - partitioned table) vs Small index size (Partitioned table)Compare the size-- give you the size of the relationSELECT pg_relation_size(oid), relnameFROM pg_classORDER BY pg_relation_size(oid) DESC;Non-partitioned tableLarge data and index sizePartitioned tableSmaller indexes size guarantee better performance on index scan query!ğŸ“ Tips Always enable partition prunning!SET enable_partition_pruning = on; -- default onIf you off that option, query will scan all partitions across whole table.Pros and Cons of PartitioningPros Improves query performance Lower size data (index size also) -&gt; Fast query performance Easy bulk loading You donâ€™t need caring about to which partition the data belongs. DB automatically decides it. Archive old data efficiently If you barely query on specific partition table (ex. old data), you could archive it into cheap storage.Cons Slow update When you move entire rows from one partition to another, it should delete rows and inserting ones. Itâ€™s so slower than just updating rows. Inefficient when scan all partitions. Suppose you scan all data in table partitioned. Itâ€™s slower than a single table since it should jump from one partition to another one. Schema changes could be challenging If master table has index, child tables also do it. However, it would not be always done. The tables actually has to support this feature. " }, { "title": "Cache Strategy", "url": "/posts/cache_strategy/", "categories": "cache", "tags": "cache", "date": "2022-11-09 00:00:00 +0900", "snippet": "1. Cache AsideApplication update cache data only when cache miss occurs reading from databaseWhen to useMassive read workloadPros Have elasticity in error since cache memory is separated even though cache cluster is down Can have different data schemaCons Synchronization issueNot always sync cache data with database Slow when cache miss occurs 2. Read ThroughDatabase update cache when cache missWhen to useMassive read workloadPros Always data is synchronized if using with Write-ThroughCons Data schema should follow database entity schema First request data always meets cache miss3. Write ThroughApplication write all data in cache first and synchronized with dbWhen to useAll cache data must be synchronized.Pros Always data is synchronized between cache and DBCons All data is in cache even though which is not used. Write operation performance degradation since all write operations must be hit DB4. Write BehindWhen to useMassive write workload R/W work load Read through + Write Behind strategy =&gt; Fast R/W and can get synchronized dataPros Fast write operation Using batch write from cache to DB, can reduce db write operationsCons Not strong consistency between cache and DB Exists data loss probability when cache error occursâš™ï¸ Related toolsRedis GearsYou can use Redis Gears when to use Write-Through or Write-Behind strategy.DAXYou can use DAX when to use Read-Through + Write-Through strategy." }, { "title": "ì¸ë±ìŠ¤ ì»¬ëŸ¼ì„ ì–´ë–»ê²Œ ì„ íƒí• ê¹Œ?", "url": "/posts/How-To-Choose-Index/", "categories": "Database", "tags": "database", "date": "2022-09-09 00:00:00 +0900", "snippet": "ì¸ë±ìŠ¤ë€?ë°ì´í„°ì˜ ë…¼ë¦¬ì  ë¶„ë¥˜ì¸ë±ìŠ¤ë¥¼ ì˜ ê³ ë¥´ë ¤ë©´?1. ë¶„í¬ë„ê°€ ì¢‹ì€ ì»¬ëŸ¼ì¸ê°€?ë¶„í¬ë„ë€? ì „ì²´ ë ˆì½”ë“œì—ì„œ ì‹ë³„ ê°€ëŠ¥í•œ ìˆ˜ì— ëŒ€í•œ ë°±ë¶„ìœ¨ex) ë‚¨ë…€ ì„±ë¹„ëŠ” ë¶„í¬ë„ê°€ 50%, ë‚˜ì´ëŠ” (1~100ì„¸) 1%ë‹¤.ì¦‰, ë°±ë¶„ìœ¨ì´ ë‚®ì„ ìˆ˜ë¡ ë¶„í¬ë„ê°€ ì¢‹ë‹¤ê³  í•œë‹¤.\\[ë¶„í¬ë„ = \\frac{1}{ì‹ë³„ ê°€ëŠ¥í•œ ìˆ˜} * 100 (\\%)\\]ì„ íƒë„ (Selectivity)-- This is definition of selectivitySELECT distinct(col1) / col1 AS 'Selectivity'FROM table_nameì„ íƒë„ëŠ” 1ì— ê°€ê¹Œìš¸ ìˆ˜ë¡, ë¶„í¬ë„ëŠ” 0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ì¢‹ë‹¤. ë¶„í¬ë„ë¥¼ 1% ë‚´ë¡œ í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ë‹¤.ë§¹ì‹ í•˜ì§€ëŠ” ë§ì. í•œ í…Œì´ë¸”ì— ì „ì²´ ë°ì´í„°ê°€ 1,000ë§Œ ê±´ì´ë¼ ê°€ì •í•˜ë©´ 1%ë§Œ í•´ë„ 10ë§Œê±´ì´ë‹¤.. í•œë²ˆì— 10ë§Œê±´ì„ ì¡°íšŒí•´ì™€ì•¼ í•˜ëŠ” ì¿¼ë¦¬ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì—ì„œ í•„ìš”ë¡œ í• ê¹Œ?ë¶„í¬ë„ê°€ ë†’ì•„ë„ ì¸ë±ìŠ¤ë¡œ ì“°ì¼ ìˆ˜ ìˆëŠ” ê²½ìš°ì£¼ë¬¸ í…Œì´ë¸”ì— â€˜ë°°ì†¡ ì—¬ë¶€â€™ ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •í•˜ì.ë°°ì†¡ ì—¬ë¶€ ì»¬ëŸ¼ì˜ ê°’ì€ (true, false) ì´ê³  ë¶„í¬ë„ëŠ” 50%ì´ë‹¤.ê·¸ì¹˜ë§Œ ì¸ë±ìŠ¤ í›„ë³´ë¡œ ì ì ˆí•˜ë‹¤.ë¬¼ë¦¬ì ìœ¼ë¡œ ì‹¤ì œ ê°’ì„ í™•ì¸í–ˆì„ ë•ŒëŠ” ìµœê·¼ì— ì£¼ë¬¸í•œ ì£¼ë¬¸ ê±´ì— ëŒ€í•´ì„œë§Œ false ì´ê³  ë‹¤ë¥¸ ëŒ€ë¶€ë¶„ì˜ ì£¼ë¬¸ê±´ì€ true ì¼ ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. ì¸ë±ìŠ¤ì˜ í›„ë³´ëŠ” ë¬¼ë¦¬ì ìœ¼ë¡œë„ íŒë‹¨í•˜ë¼.SELECT *FROM orders-- 'N' ì£¼ë¬¸ê±´ì˜ ë¹„ìœ¨ì€ ì „ì²´ì—ì„œ ë§¤ìš° ì ì„ ê²ƒì´ë‹¤.-- ì£¼ë¡œ 'ì•„ì§ ë°°ë‹¬ë˜ì§€ ì•Šì€' ì£¼ë¬¸ ê±´ì— ëŒ€í•´ ì¡°íšŒê°€ ì¼ì–´ë‚˜ëŠ” ì„œë¹„ìŠ¤ë¼ë©´-- ì´ì™€ ê°™ì€ ì¿¼ë¦¬ê°€ ìì£¼ ì´ë¤„ì§ˆ ê²ƒì´ê³  ë”°ë¼ì„œ `is_delivered' ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ë“±ë¡í• ë§Œí•˜ë‹¤.WHERE is_delivered = 'N'ORDER BY ordered_date ASC;ì¼ë°˜ì ìœ¼ë¡œ seq scan (full scan) ì€ ëŠë¦¬ë‹¤ê³  ìƒê°í•˜ì§€ë§Œ í•­ìƒ ê·¸ë ‡ì§€ë§Œì€ ì•Šë‹¤. postgres ì˜ ê²½ìš° seq scan ì„ ìˆ˜í–‰í•  ë•Œ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ê°€ parallel scan í•˜ëŠ” ë§¤ì»¤ë‹ˆì¦˜ì„ ê°–ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤.2. ê°±ì‹ ì´ ìì£¼ ë°œìƒí•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ì¸ê°€?ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¸ë±ìŠ¤ëŠ” B-Tree í˜¹ì€ B+Tree êµ¬ì¡°ë¡œ ì´ë¤„ì ¸ìˆë‹¤.ì—¬ê¸°ì„œì˜ B ëŠ” â€˜Balanceâ€™ ë¥¼ ì˜ë¯¸í•œë‹¤. (Black ì´ë‚˜ Binary ê°€ ì•„ë‹ˆë‹¤.)ë”°ë¼ì„œ ì¸ë±ìŠ¤ëŠ” INSERT, DELETE, UPDATE ë˜ë©´ ë°¸ëŸ°ìŠ¤ë¥¼ ë§ì¶”ê¸° ìœ„í•œ ì˜¤ë²„í—¤ë“œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.íŠ¹íˆ, ê°€ê¸‰ì  UPDATE ê°€ ìì£¼ ì¼ì–´ë‚˜ëŠ” ì»¬ëŸ¼ì€ ì¸ë±ìŠ¤ í›„ë³´ì—ì„œ ì œì™¸í•˜ëŠ” ê²ƒì„ ê¶Œí•œë‹¤.ì—¬ê¸°ê°€ ì¢€ ì• ë§¤í•˜ë„¤â€¦ Leaf ë…¸ë“œ ì—¬ë¶€ë‘ ìƒê´€ì—†ì´ UPDATE ê°€ INSERT / DELETE ë³´ë‹¤ ë” ë§ì´ í˜ì´ì§€ ì´ë™ì´ ì¼ì–´ë‚  ê²ƒ ê°™ì€ë°? INSERT DELETE ëŠ” ì´ë¯¸ ì´ë¯¸ ì •ë ¬ëœ ê²ƒ ê¸°ì¤€ìœ¼ë¡œ key-value ìŒìœ¼ë¡œ ì›€ì§ì´ë©´ ë˜ì§€ë§Œ Update ëŠ” í˜ì´ì§€ ì´ë™ í™•ë¥ ê³¼ ì–‘ì´ ë” ë§ì„ ë“¯â€¦?ì¸ë±ìŠ¤ë¥¼ ë‹´ê³ ìˆëŠ” Leaf ë…¸ë“œì—ì„œì˜ INSERT/DELETE ëŠ” ë°¸ëŸ°ì‹± ì˜¤ë²„í—¤ë“œ ë°œìƒ í™•ë¥ ì´ ì ì§€ë§Œ, UPDATE ëŠ” ë¦¬ë°¸ëŸ°ì‹±ì´ ì¼ì–´ë‚  í™•ë¥ ì´ ë†’ê¸° ë•Œë¬¸ì´ë‹¤.UPDATE ì‹œ ë§¤ìš° ì ì€ ì»¬ëŸ¼ì—ì„œë§Œ ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚˜ë„ë¡ í•˜ì. ë§ì€ ì»¬ëŸ¼ì—ì„œ ì—…ë°ì´íŠ¸ê°€ ì¼ì–´ë‚  ìˆ˜ë¡ ë¦¬ë°¸ëŸ°ì‹± ë¹„ìš©ì´ ë¹ ë¥´ê²Œ ì¦ê°€í•œë‹¤.3. ì¡°ê±´ì ˆì—ì„œ ìì£¼ ì‚¬ìš©ë˜ë‚˜?4. ì¡°ì¸ì˜ ì—°ê²°ê³ ë¦¬ì— ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ì¸ê°€?5. ì†ŒíŠ¸ ë°œìƒì„ ì œê±°í•˜ëŠ” ì»¬ëŸ¼ì¸ê°€?ì¸ë±ìŠ¤ ìƒì„±ë„ì™€ ì‹¤ì „ ì¸ë±ìŠ¤ì¸ë±ìŠ¤ ì¢…ë¥˜ì™€ ì‚¬ìš© ëª©ì ğŸ”— ì°¸ì¡° update use index - luke.com parallel sequential scan in postgres" }, { "title": "How to decide composite index", "url": "/posts/Composite_Index/", "categories": "Database", "tags": "database", "date": "2022-09-09 00:00:00 +0900", "snippet": "What kind of columns are to be included in composite index?Let me introduce my principal about how to choose column of composite index.1. WHERE clause2. Always used by â€˜=â€™If leading colum condition is not â€˜equal(=)â€™ next columns would not use index scan.SELECT col1, col2, col3, col4FROM table_nameWHERE col1 = ? AND col2 = ? -- col1 ~ col3 are to use index scan but col4 be not. (just filtering) -- since not equal ('=') operation is executed before. BETWEEN col3 ? AND ? AND col4 = ?3. Lower selectivity column is leadingBecause real worldâ€™s queries are executed like below. \\ Main category -&gt; middle category -&gt; subcategory4. Location column (condition, ex. ID) is followed by order column (ex. DATE)5. Use IN instead of betweenYou can take benefit of index when to use IN instead of BETWEEN IN means internally operates combination (â€˜=â€™ + OR)SELECT id, nameFROM member WITH(users_id_index) -- index ì‚¬ìš© ê°•ì œ ê°€ëŠ¥.WHERE IN id IN (1, 3, 10, 15) -- id == 1 || id == 3 || id == 10 || id == 156. Use minimum the number of columns in index7. Sync Composite key ordering with ORDER BY sequence.Sync all same sequence or on other way sequence. If not, more data access and sorting occurs." }, { "title": "Which one to choose between Postgres and MySQL", "url": "/posts/Postgres_vs_MySQL/", "categories": "Database", "tags": "database", "date": "2022-09-07 00:00:00 +0900", "snippet": "This depends on your application.Choose PostgresFrequent Write operationwhy?Postgres has secondary index architecture. So re-balancing operation cost of insert &amp; delete is cheaper than Clustered index. However, postgres create a process with large memory (almost 10 MB) per connection. This is the cause of lower speed in read operation comparing to MySQL using thread.Also, Massive read operation is inefficient by random IO because postgres only supports non-clustered index.Refer to Clustered index vs non-clustered indexChoose MySQLHigh volumes of reads. MySQL supports clustered index, so massive read is more efficient than Postgres. Random I/O is can be avoided.MySQL has two types threads, foreground and backgroundMySQLâ€™s ThreadsForeground threadRead operation from buffer or cache. Borrow a thread from thread pool. A thread request connection to Connection pool, if thereâ€™s not available (idle) connection it should wait for other thread to return the connection. âš ï¸ MySQL community edition doesnâ€™t support connection pool.Background threadUsually, Do write operation especially between buffer &lt;-&gt; disk. Merge inserting buffers. Logging to disk Monitoring Lock &amp; DeadlockğŸ”— References Postgres architecture - postgres.org Postgres vs MySQL DB Connection Pool - tistory" }, { "title": "B-Tree and B+Tree", "url": "/posts/BTree/", "categories": "Database", "tags": "database", "date": "2022-09-05 00:00:00 +0900", "snippet": "BTreeThis is the common data structure when DB store index. Each node has key - value Each node is a page N = M - 1 N: the number of element in a node M: the number of children nodesLimitation of BTree Require more space since it has key (PK) : value (RowID) ğŸ‘‰ğŸ» more nodes == more page == more IO required =&gt; âš ï¸ Make slower! Range based queries are slow because of random access Thereâ€™s a lot of disk I/O jump even though PKs are sorted. This limitation is the reason the B+Tree appearsB+TreeActually, each node has multiple key-values. youâ€™d remember the page size is 8KB or 16KB that is different according to DBMS.Whatâ€™s difference? Each root, internal nodes have only keys. ğŸ‘‰ğŸ» more elements =&gt; lower pages =&gt; lower I/O =&gt; Faster! Each leaf node has key-value value is a pointer to row (Row ID) Leaf nodes are linkedOnce a leaf node is found, there is no need to check from the tree root ğŸ‘‰ğŸ» This makes range-based query faster.Efficient range-based queryCheck points in B+Tree Most DBMS use B+Tree even MongoDB Indexes are located and fit in In-memory Leaf nodes can live in data files in the heap. Leaf nodes are linked, and the cost of linking is cheap." }, { "title": "When you meet huge table", "url": "/posts/Huge_table/", "categories": "Database", "tags": "database", "date": "2022-08-24 00:00:00 +0900", "snippet": "When you work with huge table[CASE I] Brute forceBrute forcing your way to work on the table.If you tried to find a row in the table without indexingwhich is do multi-processing, multi-threading parallely.[CASE II] IndexingCan I avoid processing entire table? ğŸ“ Best approach is indexing[CASE III] PartitioningPartitioning the table on disk breaking into smaller size, multiple parts Separate location of data logically.[CASE IV] ShardingHave multiple hosts. Divide disk , Separate data physically. You can reduce the size of table physically but complexity would increase.(Size descending) Shard -&gt; Partition -&gt; IndexConclusionBefore applying like this solution, Avoid having a billion row table. Thatâ€™s the first thingYou canâ€™t avoid it? Use indexing, partitioning, sharding. (Like DynamoDB)[Twitter]Single profile table,id, name, follower integer, follower , following on the write level.Update these things everytime someone follow / following" }, { "title": "Indexing with query planner", "url": "/posts/indexing_sql_query_planner/", "categories": "Database", "tags": "database", "date": "2022-08-17 00:00:00 +0900", "snippet": "ConfigurationTested on DBMS: PostgreSQL 14.4Table DDLcreate table users( id integer generated always as identity constraint user_id_pk primary key, name varchar(15));Suppose already inserted 1 million rows in the table.Seq scanQuerying with no index column seq scan executed.EXPLAIN (analyze, buffers) SELECT id, nameFROM usersWHERE name = 'PLVCE UHIP';ResultGather (cost=1000.00..12129.43 rows=1 width=15) (actual time=0.437..43.940 rows=1 loops=1)Workers Planned: 2Workers Launched: 2Buffers: shared hit=5668# ğŸ’¡Sqeq scan occurs!-&gt; Parallel Seq Scan on users (cost=0.00..11129.33 rows=1 width=15) (actual time=23.162..36.439 rows=0 loops=3)Filter: ((name)::text = 'PLVCE UHIP'::text)Rows Removed by Filter: 349525Buffers: shared hit=5668Planning:Buffers: shared hit=4Planning Time: 0.081 ms# ğŸ’¡Slow queryExecution Time: 43.969 msUse index scan for efficient query-- create index on `name`CREATE INDEX users_name_idx on users(name);-- DBMS decide to use 'index scan' since it has index column (name)EXPLAIN (analyze, buffers) SELECT id, nameFROM usersWHERE name = 'PLVCE UHIP';Result# âœ… Index scan occurs!Index Scan using users_name_idx on users (cost=0.42..8.44 rows=1 width=15) (actual time=0.020..0.021 rows=1 loops=1) Index Cond: ((name)::text = 'PLVCE UHIP'::text) Buffers: shared hit=4Planning: Buffers: shared hit=85Planning Time: 0.358 ms# âœ… Fast query!Execution Time: 0.039 msDonâ€™t use Like â€˜%â€™ query if you want fast query in generallike cause slow query since actually asking for â€˜expressionâ€™ not a single â€˜valueâ€™. So, seq scan should be executed instead of index scan even if it has index.EXPLAIN (analyze, buffers) SELECT id, nameFROM usersWHERE name LIKE '%PLVCE UHIP%';ResultGather (cost=1000.00..12139.83 rows=105 width=15) (actual time=0.771..42.900 rows=1 loops=1) \\Workers Planned: 2Workers Launched: 2Buffers: shared hit=5668# ğŸ’¡ Seq scan occurs!-&gt; Parallel Seq Scan on users (cost=0.00..11129.33 rows=44 width=15) (actual time=23.480..36.384 rows=0 loops=3) \\Filter: ((name)::text ~~ '%PLVCE UHIP%'::text)Rows Removed by Filter: 349525Buffers: shared hit=5668Planning:Buffers: shared hit=6Planning Time: 0.117 ms# ğŸ’¡ Slow queryExecution Time: 42.916 msTerms of query plannerGather (cost=1000.00..12129.43 rows=1 width=15) (actual time=1.664..59.150 rows=1 loops=1)Buffers: shared hit=5668Planning:Buffers: shared hit=6# ...Whatâ€™s buffers shared â€˜hitâ€™ and â€˜readâ€™?sql internally use OS cache. You can check the cache size on the shared_buffers (default: 128MB)Shared HitBuffers: shared hit = 4 means that 4 pages (each 8KB) had to be read from the main memory.Shared ReadBuffers: shared read = 4 means that 4 pages had to be read from the hard diskCost1st value 1000.00 means it took 1000 millisecond to reach at first page on heap. 2nd value 12129.43 means total amount of which took 12129.43 millisecond to read data from heap. This is not actual time, just estimated value.rowsNot an accurate number just statistical, approximate number âš ï¸ Use this value instead of COUNT() function since it would kill performance.WidthSum of all bytes for all the columns. Larger this value, larger TCP packet size." }, { "title": "Bitmap Scan", "url": "/posts/bitmap_index_scan/", "categories": "Database", "tags": "database", "date": "2022-08-17 00:00:00 +0900", "snippet": "ConfigurationDBMS : PostgreSQL 14.4Table Schemacreate table users( id integer generated always as identity constraint user_id_pk primary key, name varchar(15), score integer);-- score range [0, 100]CREATE INDEX users_score_idx ON users USING btree(score) INCLUDE (id);Suppose already inserted 1 million rows in the table.Bitmap scanWhen to useExpecting the result size is large, but need using index. DBMS would decides use bitmap scan.EXPLAIN ANALYZE SELECT nameFROM usersWHERE score &gt; 90;Resultâœ… Bitmap Scan!Bitmap Heap Scan on users (cost=1962.80..9948.87 rows=104565 width=10) (actual time=28.306..99.511 rows=104072 loops=1) Recheck Cond: (score &gt; 90) Heap Blocks: exact=6679 -&gt; Bitmap Index Scan on users_score_idx (cost=0.00..1936.66 rows=104565 width=0) (actual time=27.578..27.578 rows=104072 loops=1) Index Cond: (score &gt; 90)Planning Time: 0.076 msExecution Time: 103.958 msHow it operates?Postgres makes a bitmap . When index condition is true in the specific page, marking true bit on bitmap (page number)Not going to jump directly to the heap as finding index.Finally, after all marking the bitmap, can jump once to the heap table and pull all these pages.ğŸ‘‰ğŸ» Lower page jump in heap, more efficient" }, { "title": "Clustered index vs non-clustered index", "url": "/posts/Clustered_vs_Non-Clustered/", "categories": "Database", "tags": "database", "date": "2022-08-16 00:00:00 +0900", "snippet": "Primary keyOrganized, clustered table. we call it Index Organized Table (IOT) Clustered index that is sequential is not that bad. So, itâ€™s ordered data and can use efficient caching mechanism. If not, memory in heap would jump left and right, That will not benefit of memory caching DB is doing.Secondary KeyThis is used in non-clustered index. Have an additional outside index as B-Tree. Itâ€™s not in table. âš ï¸ Postgres doesnâ€™t have primary index. All indexes are secondary index. (Non-clustered index)Non-Clustered Index Located in separated memory section. So, should jump to HEAP One or more exists in a table. Store value and a pointer to actual row that holds data. The leaf node has only included columns. Just logical order, not organized physically. Keep Row ID (RID) in non-leaf level B+Tree. Composite key when used with unique constraints of the table act as non-clustered index.How to create non-clustered index in DBMSWhen to use non-clustered index architecture OLTP - character Query Selectivity almost close to 1 Using covering index Non-clustered index also sorted and stored in separate from heap space. Massive aggregation function (by only index scan)Secondary index in Postgres and MySQL âš ï¸ Donâ€™t use big primary key in MySQL using secondary index because it keeps primary key in secondary indexWhen create non-clustered index, automatically clustered index columns are added additionally.Heap tableTable with no clustered index. (Only data page)Clustered IndexWhen creating primary key(clustered index) is physically and logically sorted and with entire row data in data page (Heap). Each page is connected with doubly linked list. However, clustered index table doesnâ€™t guarantee physical order of rows as time goes by. Because INSERT, UPDATE, DELETE operations make a page split. Each age gets fragmented On the other side, logical order of pages is sustainable with link.Page splitIn RDBMS, a table exists having auto increment Primary key 1, 3, 5, 7â€¦ You insert primary key â€˜2â€™ record on the table. What happened? If page have empty space =&gt; insert the row in that page and update page offset. If page doesnâ€™t have empty space =&gt; split 50% data to a new page Page split have impact on OLAP since thereâ€™s many empty page I/O. However, doesnâ€™t large impact on OLTP (low page I/O)Why need index rebuild?When rebuilding clustered index, fragments from page split are removed.After rebuilding, Pages and rows are organized physically , logically both.Tips Donâ€™t create clustered index on running table. That work is so hard workloadWhen to use clustered index OLAP - character query Performance 10 times than non-clustered index when you request query for massive data. Massive join ORDER BY clause GROUP BY clauseIn MySQL, MSSQL, clustered index is only Primary Key.How to choice Primary key (Clustered index) Unique column Small key size Static column Should not be updated, If not, page split occurs or re-balancing occurs âš ï¸ In Index column, thereâ€™s only delete &amp; insert solution for re-balancing Not exists In-place update.âœ… This is auto_increment! But itâ€™s not absolute solution.ğŸ”— Reference Difference between clustered and non clustered index - GeeksForGeeks Clustered index do not guarantee order of rows" }, { "title": "Required terms of database", "url": "/posts/page_and_IO/", "categories": "Database", "tags": "database", "date": "2022-08-16 00:00:00 +0900", "snippet": "PagesThe logical unit, that of database stores and reads the data. Different according to DBMS (ex. PostgreSQL has a 8KB page, MySQL does 16KB) DB handle data one or more pages in a single IO.IO (physical IO)Operation unit a request to disk. Considered performance, developers should minimize this since this is expensive (on disk). Some OS(ex. PostgreSQL) use OS cache instead of disk.Logical I/OOperation request to shared_buffers, os cache etc..IndexPhysical method of storing data. This is for searching data fast Consists of B-Tree data structure. Stored as pages.B-Tree search operation Time Complexity is Log(N)HeapEverything is stored in here (disk). More IO in this section, more cost and slower the queries." }, { "title": "Strong vs Eventual Consistency", "url": "/posts/eventual_consistency/", "categories": "Database", "tags": "database", "date": "2022-08-16 00:00:00 +0900", "snippet": "Strong Consistencyâ€œAlways all data is to have integrityâ€Eventual ConsistencyThis ensures all read operations retrieves updated data at the end.Master Node + Read replica Index Strong Consistency Eventual Consistency Pros Data consistency and integrity Scalability, Performance Cons Scalability, Performance Weak consistency and integrity When To Use Small data &lt;/br&gt;context is specific Large data&lt;/br&gt;context is general Example use-case Payment transaction Likes on instagram picture What developers have to decide?Always consider â€˜user-centeredâ€™ perspective, Does it affect on user experience?ğŸ”— References Balancing strong and eventual consistency - Google Cloud Platform" }, { "title": "ACID", "url": "/posts/ACID/", "categories": "Database", "tags": "database", "date": "2022-08-11 00:00:00 +0900", "snippet": "ACID principal Atomicity Consistency Isolation DurabilityAtomicityA minimum unit of job which canâ€™t be split. If any failure, all queries in a transaction should be rollback .IsolationDirty readTwo transactions tx1, tx2 are executing in parallel Tx1 read something Tx2 write something When Tx1 read something after tx2 write something but tx2 rollback afterwards. Tx1 might read consistent data that is dirty valueThis is dirty read.Non-repeatable readAfter tx2 write something, tx1 read after tx2 commit(). So tx1 read updated and inconsistent value Tx1 should only read the status of data at the specific point for ensuring consistent.Phantom readAfter tx2 write something, tx1 read after tx2 commit(). so tx1 read new value. Tx1 should only read the status of data at the specific point for ensuring consistent.Lost updateTx1 update some record and at the same time, Tx1 update same record. One of them, one transactionâ€™s update should be abandoned.Isolation LevelRead uncommittedNo isolation, any change from the outside is visible to the transaction, regardless of commit() Dirty read is allowed, although itâ€™s fast.Read committedEach query can only see after outside transaction is committed. Dirty read is denied.Repeatable ReadThe read data remain unchanged while reading transaction is running. This denys non-repeatable read.SnapshotTransaction can only see changes that have been committed from the start of the transaction. This resolves every read isolation phenomenon.SerializableAll transactions must be executed isolated. Isolation Level Dirty read Non-repeatable read Phantom Lost Update Read Uncommitted may occur may occur may occur may occur Read Committed donâ€™t occur may occur may occur may occur Repeatable Read donâ€™t occur donâ€™t occur may occur donâ€™t occur Serializable donâ€™t occur donâ€™t occur donâ€™t occur donâ€™t occur âš ï¸In Repeatable Read, when a row inserted in the middle of transaction but new record is not locked and visible to transaction.So phantom may occur.DBMS implements isolation level differently.ConsistencyDefined by DBA, Referential integrity (Foreign key)Eventual consistencyAllow some inconsistency like below. Master node (Write Only) and slave node (Read Only) has some data inconsistency during synchronization.DurabilityThis make DBMS non-volatile, persisted system even if thereâ€™s a rash. Data is persistent and has failover mechanism.Write Ahead Log (WAL)Write data in write-ahead-log segments (compressed version). DB keeps change logs in undo logs in replica status. It makes slowerOS CacheA write request in OS usually goes to the OS cache since it wants flush many data reducing the number of dispatch to disk. This decides where a transaction writes the data in OS cache or disk.Asynchronous snapshotAppend Only File (AOF)Redis durability Redis support Asynchronous snapshot and AOF both. Snapshots in redis are generated in background process which is forked.This is quite expensive.AOF is slower than snapshots since the size of the AOF file is continuously going to grow over time" } ]
